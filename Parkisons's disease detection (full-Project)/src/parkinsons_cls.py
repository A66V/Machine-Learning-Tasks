# -*- coding: utf-8 -*-
"""Parkinsons_cls.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13MYmoWqFJtu9U72_A2b5l-a8YHjwgeQY

##Importing Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import ast
import time
import pickle
import scipy.stats as stats
from sklearn import preprocessing
from sklearn.compose import ColumnTransformer
from sklearn.feature_selection import f_regression
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeRegressor
from sklearn.tree import plot_tree
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso, Ridge
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler

"""##Importing Dataset"""

df2 = pd.read_csv('parkinsons_disease_data_cls.csv')
df2.head()

"""##Data Exploration"""

#Number of Nulls
print(df2.isnull().sum())
#percentage of nulls
df2.isnull().sum()/df2.shape[0]*100

#Finding Duplicates
df2.duplicated().sum()

#Finding Garbage Values
for i in df2.select_dtypes(include=['object']).columns:
    print(df2[i].value_counts())
    print("***"*10)

"""##Data Preprocessing"""

#Handle Missing Values
mode_education = df2['EducationLevel'].mode()[0]
df2['EducationLevel'] = df2['EducationLevel'].fillna(mode_education)
df2.head()

#Handle Outliers for numeric features only
columns = df2.select_dtypes(include=['float64', 'int64']).columns
for i in columns:
    q1 = np.percentile(df2[i], 25)
    q3 = np.percentile(df2[i], 75)
    norm_range = (q3 - q1) * 1.5
    # Identify lower outliers
    lower_outliers = df2[df2[i] < (q1 - norm_range)]
    # Identify upper outliers
    upper_outliers = df2[df2[i] > (q3 + norm_range)]
    # Count the total number of outliers
    outliers = len(lower_outliers) + len(upper_outliers)
    print(f"The number of outliers in {i}: {outliers}")
    # Replace outliers with adjusted values
    df2[i] = np.where(df2[i] < (q1 - norm_range), q1 - norm_range, df2[i])
    df2[i] = np.where(df2[i] > (q3 + norm_range), q3 + norm_range, df2[i])
    # Count the total number of outliers after replacment
    outliers = len(lower_outliers) + len(upper_outliers)
    print(f"The number of outliers in {i} after replacment: {outliers}")
    print("***"*20)

#Feature Engineering / feature creation
df2['PulsePressure'] = (df2['SystolicBP'] - df2['DiastolicBP'])
df2.drop(['SystolicBP', 'DiastolicBP'], axis=1, inplace=True)
df2['ldl/hdl'] = (df2['CholesterolLDL'] / df2['CholesterolHDL'])
df2['tg/hdl'] = (df2['CholesterolTriglycerides'] / df2['CholesterolHDL'])
df2.drop(['CholesterolLDL', 'CholesterolHDL','CholesterolTriglycerides'], axis=1, inplace=True)

#Handle Object-like Features
# Convert string to dictionary (if stored as JSON-like string)
df2['MedicalHistory'] = df2['MedicalHistory'].apply(ast.literal_eval)
df2['Symptoms'] = df2['Symptoms'].apply(ast.literal_eval)

# Split into separate columns
medical_history_expanded = df2['MedicalHistory'].apply(pd.Series)
Symptoms_expanded = df2['Symptoms'].apply(pd.Series)

# Combine with original DataFrame
df2 = pd.concat([df2.drop('MedicalHistory', axis=1), medical_history_expanded], axis=1)
df2 = pd.concat([df2.drop('Symptoms', axis=1), Symptoms_expanded], axis=1)

df2.head()
df2.info()

#Handle WeeklyPhysicalActivity column (Change from HH:MM formate to Numerical formate)

df2['WeeklyPhysicalActivity (hr)'] = df2['WeeklyPhysicalActivity (hr)'].str.split(':').apply(lambda x: float(x[0]) + (float(x[1]) / 60) )# Convert minutes to fractional hours
df2['WeeklyPhysicalActivity (hr)']

"""Saving Impuation values"""

# Calculate and save imputation values from training data
imputation_values = {}

# For numeric features
numeric_features = df2.select_dtypes(include=['int64', 'float64']).columns
for col in numeric_features:
    imputation_values[col] = df2[col].median()

# For categorical features
categorical_features = df2.select_dtypes(include=['object']).columns
for col in categorical_features:
    imputation_values[col] = df2[col].mode()[0]

# Save imputation values
with open('imputation_values.pkl', 'wb') as f:
    pickle.dump(imputation_values, f)

print(imputation_values)

#Normalization
# Select numerical features for normalization
numerical_features = df2.select_dtypes(include=['int64', 'float64']).columns.difference(['PatientID']).tolist()
if "Diagnosis" in numerical_features:
    numerical_features.remove("Diagnosis")

# Exclude the target variable 'UPDRS' from normalization

print(numerical_features)

categorical_features  = df2.select_dtypes(include=['object']).columns.tolist()

#print("All columns:", list(df))
print("Numerical columns:", numerical_features)
print("Categorical columns:", categorical_features)

# Create a MinMaxScaler object
scaler = preprocessing.MinMaxScaler()
y_scaler=preprocessing.MinMaxScaler()


# Fit and transform the numerical features
df2[numerical_features] = scaler.fit_transform(df2[numerical_features])


#Save the Scalar
with open('minmax_scaler_cls.pkl', 'wb') as f:
    pickle.dump(scaler, f)

# Display the normalized data
with pd.option_context('display.max_columns', None):
    print(df2.head())

#Label encoding

# Create a dictionary to store label encoders for each column
label_encoders = {}

cols = df2.select_dtypes(include=['object']).columns
for i in cols:
    label_encoding=preprocessing.LabelEncoder()
    df2[i]=label_encoding.fit_transform(df2[i])
    # Store the fitted label encoder in the dictionary with the column name as the key
    label_encoders[i] = label_encoding

# Save the dictionary of label encoders after the loop
with open('label_encoders.pkl', 'wb') as f:
    pickle.dump(label_encoders, f)

# Now you can load the dictionary of label encoders correctly
with open('label_encoders.pkl', 'rb') as f:
    loaded_label_encoders = pickle.load(f) # Use a different variable name to avoid confusion

print(loaded_label_encoders)

#with pd.option_context('display.max_columns', None):
    #print(df2.info())
    #print(df2.head())

"""## Feature Selection"""

_y_=df2['Diagnosis']
_x_num=df2[numerical_features]

_f_scores, _p_values = f_regression(_x_num, _y_)

_anova_results = pd.DataFrame({
    'Feature': numerical_features,
    'F_score': _f_scores,
    'p_value': _p_values
})

# Print the ANOVA results
print("\nANOVA Test Results for Categorical Features:")
print(_anova_results.sort_values('p_value'))

# 8. Select important categorical features based on p-value
_significance_level = 0.1
_selected_numerical_features = _anova_results[_anova_results['p_value'] < _significance_level]['Feature'].tolist()

print("\nSelected important categorical features (p-value < 0.1):")
print(_selected_numerical_features)

from sklearn.feature_selection import mutual_info_classif

_x_cat=df2[categorical_features]

_mutual_info_scores = mutual_info_classif(_x_cat, _y_,random_state=42)

_mutual_info_results = pd.DataFrame({
    'Feature': categorical_features,
    'Mutual_Info_Score': _mutual_info_scores
})

print("\nMutual Information Scores for Categorical Features:")
print(_mutual_info_results.sort_values('Mutual_Info_Score', ascending=False))

_significance_level2 = 0.015
_selected_categorical_features = _mutual_info_results[_mutual_info_results['Mutual_Info_Score'] > _significance_level2]['Feature'].tolist()

_final_selected_features=_selected_categorical_features+_selected_numerical_features
print(_final_selected_features)
with open('final_features.pkl', 'wb') as f:
    pickle.dump(_final_selected_features, f)

# Define x (input features)
x_cf = df2[_final_selected_features]

# Define y (target variable)
y_cf = df2['Diagnosis']

"""## SVM"""

xTrain, xTest, yTrain, yTest = train_test_split(x_cf, y_cf, test_size = 0.2, random_state = 42)

# Hyperparameters grid for the SVM Model
parametersGrid = {
    'C': [0.1, 1, 10, 100],              # Regularization strength
    'gamma': ['scale', 'auto', 0.01, 0.001, 0.0001],  # Kernel coefficient
    'kernel': ['rbf', 'linear', 'poly']   # Different kernel types
}

svmModel = SVC(random_state=42)

# Applying Grid Search to find the best hyperparameters
grid_search = GridSearchCV(estimator = svmModel, param_grid = parametersGrid, scoring = 'accuracy', cv = 5, n_jobs = -1, verbose = 2)

startTrainTime = time.time()
grid_search.fit(xTrain, yTrain)
endTrainTime = time.time()

# Training time = End of training's timer time - Start of training's timer time
trainingTime = endTrainTime - startTrainTime

# Best model after Grid Search
svmModelAfterGridSearch = grid_search.best_estimator_

#Save Model after gird search
with open('svm_model.pkl', 'wb') as f:
    pickle.dump(svmModelAfterGridSearch, f)

startTestTime = time.time()
yPrediction = svmModelAfterGridSearch.predict(xTest)
endTestTime = time.time()

# Testing time = End of testing's timer time - Start of testing's timer time
testingTime = endTestTime - startTestTime

# Evaluation metrics
accuracy = accuracy_score(yTest, yPrediction)
confusionMatrix = confusion_matrix(yTest, yPrediction)
classificationReport = classification_report(yTest, yPrediction)

print("SVM Model Results:\n")

print("-- Cross Validation Results --")
print("Best Hyperparameters:", grid_search.best_params_)


SVM_Accuracy = round(accuracy * 100, 2)
print("\n-- Test Results --")
print("Accuracy: ", SVM_Accuracy, "%")
print("\n-- Classification Report --")
print(classificationReport)

# Training and testing times

SVM_TrainTime = round(trainingTime, 2)
SVM_TestTime = round(testingTime, 2)
print("\n-- Performance --")
print("Training Time: ", SVM_TrainTime, "seconds")
print("Testing Time: ", SVM_TestTime, "seconds")

# Plotting confusion matrix for SVM
plt.figure(figsize=(6,5))
sns.heatmap(confusionMatrix, annot = True, fmt = 'd', cmap = 'Blues')
plt.title('Confusion Matrix (SVM) Heatmap')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

"""## Logistic Regression

"""

xTrain, xTest, yTrain, yTest = train_test_split(x_cf, y_cf, test_size = 0.2, random_state = 42)

parametersGrid = [
    # For l2 penalty-> all solvers work
    {'penalty': ['l2'], 'solver': ['liblinear', 'saga', 'lbfgs'], 'C': [0.1, 1, 10, 100]},

    # For l1 penalty -> liblinear and saga only work
    {'penalty': ['l1'], 'solver': ['liblinear', 'saga'], 'C': [0.1, 1, 10, 100]},

    # For'elasticnet penalty -> saga only works + l1 ratio required for it
    {'penalty': ['elasticnet'], 'solver': ['saga'], 'C': [0.1, 1, 10, 100], 'l1_ratio': [0.1, 0.2, 0.5, 0.7, 0.9]}
]

logisticModel = LogisticRegression(random_state=42)

# Applying Grid Search to find the best hyperparameters
grid_search = GridSearchCV(estimator = logisticModel, param_grid = parametersGrid, scoring = 'accuracy', cv = 5, n_jobs = -1, verbose = 2)

startTrainTime = time.time()
grid_search.fit(xTrain, yTrain)
endTrainTime = time.time()

# Training time = End of training's timer time - Start of training's timer time
trainingTime = endTrainTime - startTrainTime

# Best model after Grid Search
logisticModelAfterGridSearch = grid_search.best_estimator_

startTestTime = time.time()
yPrediction = logisticModelAfterGridSearch.predict(xTest)
endTestTime = time.time()

#Save Model
with open('logistic_regression_model.pkl', 'wb') as f:
    pickle.dump(logisticModelAfterGridSearch, f)

# Testing time = End of testing's timer time - Start of testing's timer time
testingTime = endTestTime - startTestTime

# Evaluation metrics
accuracy = accuracy_score(yTest, yPrediction)
confusionMatrix = confusion_matrix(yTest, yPrediction)
classificationReport = classification_report(yTest, yPrediction)

# Displaying Results
print("Logistic Regression Model Results:\n")

print("-- Cross Validation Results --")
print("Best Hyperparameters:", grid_search.best_params_)

Logistic_Accuracy = round(accuracy * 100, 2)
print("\n-- Test Results --")
print("Accuracy: ", Logistic_Accuracy, "%")
print("\n-- Classification Report --")
print(classificationReport)

# Training and testing times
print("\n-- Performance --")

Logistic_TrainTime = round(trainingTime, 2)
Logistic_TestTime = round(testingTime, 2)
print("Training Time: ", Logistic_TrainTime, "seconds")
print("Testing Time: ", Logistic_TestTime, "seconds")

# Plotting confusion matrix for Logistic Regression
plt.figure(figsize=(6,5))
sns.heatmap(confusionMatrix, annot = True, fmt = 'd', cmap = 'Blues')
plt.title('Confusion Matrix (Logistic Regression) Heatmap')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

"""## Random Forest

"""

from sklearn.ensemble import RandomForestClassifier

xTrain, xTest, yTrain, yTest = train_test_split(x_cf, y_cf, test_size=0.2, random_state=42)

# Hyperparameters grid for the Random Forest Model
parametersGrid = {
    'n_estimators': [50, 100, 200],       # Number of trees
    'max_depth': [None, 10, 20, 30],      # Maximum depth of trees
    'min_samples_split': [2, 5, 10],      # Minimum samples to split a node
    'min_samples_leaf': [1, 2, 4],        # Minimum samples at a leaf node
    'bootstrap': [True, False]            # Whether bootstrap samples are used
}

rfModel = RandomForestClassifier(random_state=42)

# Applying Grid Search to find the best hyperparameters
grid_search = GridSearchCV(estimator=rfModel, param_grid=parametersGrid, scoring='accuracy', cv=5, n_jobs=-1, verbose=2)

startTrainTime = time.time()
grid_search.fit(xTrain, yTrain)
endTrainTime = time.time()

# Training time = End of training's timer time - Start of training's timer time
trainingTime = endTrainTime - startTrainTime

# Best model after Grid Search
rfModelAfterGridSearch = grid_search.best_estimator_

#save model
with open('rf_model.pkl', 'wb') as f:
    pickle.dump(rfModelAfterGridSearch, f)

startTestTime = time.time()
yPrediction = rfModelAfterGridSearch.predict(xTest)
endTestTime = time.time()

# Testing time = End of testing's timer time - Start of testing's timer time
testingTime = endTestTime - startTestTime

# Evaluation metrics
accuracy = accuracy_score(yTest, yPrediction)
confusionMatrix = confusion_matrix(yTest, yPrediction)
classificationReport = classification_report(yTest, yPrediction)

print("Random Forest Model Results:\n")

print("-- Cross Validation Results --")
print("Best Hyperparameters:", grid_search.best_params_)

print("\n-- Test Results --")
RF_Accuracy = round(accuracy * 100, 2)
print("Accuracy: ", RF_Accuracy, "%")
print("\n-- Classification Report --")
print(classificationReport)

# Training and testing times
print("\n-- Performance --")
RF_TrainTime = round(trainingTime, 2)
RF_TestTime = round(testingTime, 2)
print("Training Time: ", RF_TrainTime, "seconds")
print("Testing Time: ", RF_TestTime, "seconds")

# Plotting confusion matrix for Random Forest
plt.figure(figsize=(6,5))
sns.heatmap(confusionMatrix, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix (Random Forest) Heatmap')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

"""##Bar Graph Construciton"""

metrics = {
    'Model': ['SVM', 'Logistic Regression', 'Random Forest'],
    'Accuracy (%)': [SVM_Accuracy, Logistic_Accuracy, RF_Accuracy],
    'Training Time (s)': [SVM_TrainTime, Logistic_TrainTime, RF_TrainTime],
    'Test Time (s)': [SVM_TestTime, Logistic_TestTime, RF_TestTime]
}

df_metrics = pd.DataFrame(metrics)

plt.figure(figsize=(8, 5))
plt.bar(df_metrics['Model'], df_metrics['Accuracy (%)'], color=['skyblue', 'lightgreen', 'salmon'])
plt.title('Model Comparison: Classification Accuracy')
plt.ylabel('Accuracy (%)')
plt.ylim(0, 100)
plt.grid(axis='y', linestyle='--')
plt.show()

plt.figure(figsize=(8, 5))
plt.bar(df_metrics['Model'], df_metrics['Training Time (s)'], color=['skyblue', 'lightgreen', 'salmon'])
plt.title('Model Comparison: Training Time')
plt.ylabel('Time (seconds)')
plt.grid(axis='y', linestyle='--')
plt.show()

plt.figure(figsize=(8, 5))
plt.bar(df_metrics['Model'], df_metrics['Test Time (s)'], color=['skyblue', 'lightgreen', 'salmon'])
plt.title('Model Comparison: Test Time')
plt.ylabel('Time (seconds)')
plt.grid(axis='y', linestyle='--')
plt.show()

"""-------------------------------------------------------------------------------------------------------------------"""

# import time
# import numpy as np
# from sklearn.linear_model import LogisticRegression, LinearRegression, Lasso, Ridge
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.pipeline import make_pipeline
# from sklearn.preprocessing import PolynomialFeatures
# from sklearn.model_selection import train_test_split, GridSearchCV
# from sklearn.metrics import (
#     mean_squared_error, mean_absolute_error, r2_score,
#     accuracy_score, confusion_matrix, classification_report
# )

# def time_and_evaluate_regression(name, model, X_train, y_train, X_test, y_test):
#     t0 = time.perf_counter()
#     model.fit(X_train, y_train)
#     train_time = time.perf_counter() - t0

#     t1 = time.perf_counter()
#     preds = model.predict(X_test)
#     test_time = time.perf_counter() - t1

#     mse = mean_squared_error(y_test, preds)
#     rmse = np.sqrt(mse)
#     mae = mean_absolute_error(y_test, preds)
#     r2 = r2_score(y_test, preds)

#     print(f"{name} Regression:")
#     print(f"  Train time: {train_time:.4f} s")
#     print(f"  Test  time: {test_time:.4f} s")
#     print(f"  MAE : {mae:.4f}")
#     print(f"  RMSE: {rmse:.4f}")
#     print(f"  R²  : {r2:.4f}\n")

# def time_and_evaluate_classification(name, model, X_train, y_train, X_test, y_test):
#     t0 = time.perf_counter()
#     model.fit(X_train, y_train)
#     train_time = time.perf_counter() - t0

#     t1 = time.perf_counter()
#     preds = model.predict(X_test)
#     test_time = time.perf_counter() - t1

#     acc = accuracy_score(y_test, preds)
#     print(f"{name} Classification:")
#     print(f"  Train time: {train_time:.4f} s")
#     print(f"  Test  time: {test_time:.4f} s")
#     print(f"  Accuracy : {acc*100:.2f}%")
#     print("  Confusion Matrix:\n", confusion_matrix(y_test, preds))
#     print("  Classification Report:\n", classification_report(y_test, preds))
#     print()


# time_and_evaluate_regression(
#     "Linear", LinearRegression(),
#     X_train, y_train, X_test, y_test
# )

# time_and_evaluate_regression(
#     "Poly (deg=2)",
#     make_pipeline(PolynomialFeatures(degree=2), LinearRegression()),
#     X_train, y_train, X_test, y_test
# )

# for deg in [1, 2, 3, 4, 5]:
#     time_and_evaluate_regression(
#         f"Poly (deg={deg})",
#         make_pipeline(PolynomialFeatures(degree=deg), LinearRegression()),
#         X_train, y_train, X_test, y_test
#     )

# time_and_evaluate_regression(
#     "Lasso (α=0.1)",
#     Lasso(alpha=0.1),
#     X_train, y_train, X_test, y_test
# )

# time_and_evaluate_regression(
#     "Ridge (α=1.0)",
#     Ridge(alpha=1.0),
#     X_train, y_train, X_test, y_test
# )


# # --- Logistic Regression with GridSearch timing ---
# parametersGrid = [
#     {'penalty': ['l2'],       'solver': ['liblinear','saga','lbfgs'],               'C': [0.1,1,10,100]},
#     {'penalty': ['l1'],       'solver': ['liblinear','saga'],                      'C': [0.1,1,10,100]},
#     {'penalty': ['elasticnet'],'solver': ['saga'], 'C': [0.1,1,10,100], 'l1_ratio':[0.1,0.2,0.5,0.7,0.9]}
# ]
# logistic = LogisticRegression(random_state=42, max_iter=5000)  # sagas may need more iterations
# grid_log = GridSearchCV(logistic, parametersGrid, scoring='accuracy', cv=5, n_jobs=-1, verbose=2)

# t0 = time.perf_counter()
# grid_log.fit(xTrain, yTrain)
# train_time = time.perf_counter() - t0

# best_log = grid_log.best_estimator_

# t1 = time.perf_counter()
# y_pred_log = best_log.predict(xTest)
# test_time = time.perf_counter() - t1

# print("Logistic Regression (GridSearch):")
# print("  Best params   :", grid_log.best_params_)
# print(f"  Train time    : {train_time:.4f} s")
# print(f"  Test  time    : {test_time:.4f} s")
# print("  Accuracy      :", round(accuracy_score(yTest, y_pred_log)*100,2), "%")
# print("  Classification Report:\n", classification_report(yTest, y_pred_log), "\n")


# # --- Random Forest with GridSearch timing ---
# rf_params = {
#     'n_estimators': [50,100,200],
#     'max_depth':    [None,10,20,30],
#     'min_samples_split':[2,5,10],
#     'min_samples_leaf': [1,2,4],
#     'bootstrap':     [True, False]
# }
# rf = RandomForestClassifier(random_state=42)
# grid_rf = GridSearchCV(rf, rf_params, scoring='accuracy', cv=5, n_jobs=-1, verbose=2)

# t0 = time.perf_counter()
# grid_rf.fit(xTrain, yTrain)
# train_time = time.perf_counter() - t0

# best_rf = grid_rf.best_estimator_

# t1 = time.perf_counter()
# y_pred_rf = best_rf.predict(xTest)
# test_time = time.perf_counter() - t1

# print("Random Forest (GridSearch):")
# print("  Best params   :", grid_rf.best_params_)
# print(f"  Train time    : {train_time:.4f} s")
# print(f"  Test  time    : {test_time:.4f} s")
# print("  Accuracy      :", round(accuracy_score(yTest, y_pred_rf)*100,2), "%")
# print("  Classification Report:\n", classification_report(yTest, y_pred_rf))

"""Decision Tree"""

# cols_names = ['PatientID', 'Age', 'Gender', 'Ethnicity', 'EducationLevel', 'BMI',
#        'Smoking', 'AlcoholConsumption', 'DietQuality', 'SleepQuality',
#        'SystolicBP', 'DiastolicBP', 'CholesterolTotal', 'CholesterolLDL',
#        'CholesterolHDL', 'CholesterolTriglycerides', 'UPDRS', 'MoCA',
#        'FunctionalAssessment', 'DoctorInCharge', 'WeeklyPhysicalActivity (hr)',
#        'MedicalHistory', 'Symptoms', 'Diagnosis']

# df_uni = pd.read_csv("/content/parkinsons_disease_data_cls.csv",skiprows=1,header=None, names=cols_names)

# df_uni.head(10)

# class Node():
#     def __init__(self,feature_index=None,threshold=None,left=None,right=None,info_gain=None,value=None):
#         ''' constructor '''
#         # For decision nodes (Containg an Conditions) :
#         self.feature_index=feature_index
#         self.threshold=threshold
#         self.left=left
#         self.right=right
#         self.info_gain=info_gain

#         # For Leaf Nodes (Majority Class of the leaf Node :):
#         self.value=value

# class DecisionTreeClassifier():
#     def __init__(self,min_samples_split=2,max_depth=2):
#         ''' constructor '''

#         # initialize the root of the tree to traverse on the tree:
#         self.root = None

#         # Stopping Conditions :
#         self.min_samples_split=min_samples_split
#         self.max_depth=max_depth

#     def build_tree(self,dataset,curr_depth=0):
#         ''' recursive function to build the tree '''
#         X, Y = dataset[:,:-1], dataset[:,-1]
#         number_of_samples , number_of_features = np.shape(X)

#         # split until stopping condition are met :
#         if number_of_samples>=self.min_samples_split and curr_depth<=self.max_depth:
#             # Find the best split :
#             best_split = self.get_best_split(dataset,number_of_samples,number_of_features)
#             # Check if the info Gain is +ve  :
#             if best_split["info_gain"]>0:
#                 # recursive on the Left :
#                 left_subtree = self.build_tree(best_split["dataset_left"],curr_depth+1)
#                 # recursive on the Right :
#                 right_subtree = self.build_tree(best_split["dataset_right"],curr_depth+1)
#                 # return to decision node :
#                 return Node(best_split["feature_index"],best_split["threshold"],left_subtree,right_subtree,best_split["info_gain"])

#         #Compute the leaf value :
#         leaf_value = self.calculate_leaf_value(Y)
#         return Node(value=leaf_value)


#     def get_best_split(self, dataset, num_samples, num_features):
#         ''' function to find the best split '''

#         # dictionary to store the best split
#         best_split = {}
#         max_info_gain = -float("inf")

#         # loop over all the features
#         for feature_index in range(num_features):
#             feature_values = dataset[:, feature_index]
#             possible_thresholds = np.unique(feature_values)
#             # loop over all the feature values present in the data
#             for threshold in possible_thresholds:
#                 # get current split
#                 dataset_left, dataset_right = self.split(dataset, feature_index, threshold)
#                 # check if childs are not null
#                 if len(dataset_left)>0 and len(dataset_right)>0:
#                     y, left_y, right_y = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]
#                     # compute information gain
#                     curr_info_gain = self.information_gain(y, left_y, right_y, "gini")
#                     # update the best split if needed
#                     if curr_info_gain>max_info_gain:
#                         best_split["feature_index"] = feature_index
#                         best_split["threshold"] = threshold
#                         best_split["dataset_left"] = dataset_left
#                         best_split["dataset_right"] = dataset_right
#                         best_split["info_gain"] = curr_info_gain
#                         max_info_gain = curr_info_gain

#         # return best split
#         return best_split

#     def split(self, dataset, feature_index, threshold):
#         ''' function to split the data '''

#         dataset_left = np.array([row for row in dataset if row[feature_index]<=threshold])
#         dataset_right = np.array([row for row in dataset if row[feature_index]>threshold])
#         return dataset_left, dataset_right

#     def information_gain(self, parent, l_child, r_child, mode="entropy"):
#         ''' function to compute information gain '''

#         weight_l = len(l_child) / len(parent)
#         weight_r = len(r_child) / len(parent)
#         if mode=="gini":
#             gain = self.gini_index(parent) - (weight_l*self.gini_index(l_child) + weight_r*self.gini_index(r_child))
#         else:
#             gain = self.entropy(parent) - (weight_l*self.entropy(l_child) + weight_r*self.entropy(r_child))
#         return gain

#     def entropy(self, y):
#         ''' function to compute entropy '''

#         class_labels = np.unique(y)
#         entropy = 0
#         for cls in class_labels:
#             p_cls = len(y[y == cls]) / len(y)
#             entropy += -p_cls * np.log2(p_cls)
#         return entropy

#     def gini_index(self, y):
#         ''' function to compute gini index '''

#         class_labels = np.unique(y)
#         gini = 0
#         for cls in class_labels:
#             p_cls = len(y[y == cls]) / len(y)
#             gini += p_cls**2
#         return 1 - gini

#     def calculate_leaf_value(self, Y):
#         ''' function to compute leaf node '''

#         Y = list(Y)
#         return max(Y, key=Y.count)

#     def print_tree(self, tree=None, indent=" "):
#         ''' function to print the tree '''

#         if not tree:
#             tree = self.root

#         if tree.value is not None:
#             print(tree.value)

#         else:
#             print("X_"+str(tree.feature_index), "<=", tree.threshold, "?", tree.info_gain)
#             print("%sleft:" % (indent), end="")
#             self.print_tree(tree.left, indent + indent)
#             print("%sright:" % (indent), end="")
#             self.print_tree(tree.right, indent + indent)

#     def fit(self, X, Y):
#         ''' function to train the tree '''

#         dataset = np.concatenate((X, Y), axis=1)
#         self.root = self.build_tree(dataset)

#     def predict(self, X):
#         ''' function to predict new dataset '''

#         preditions = [self.make_prediction(x, self.root) for x in X]
#         return preditions

#     def make_prediction(self, x, tree):
#         ''' function to predict a single data point '''

#         if tree.value!=None: return tree.value
#         feature_val = x[tree.feature_index]
#         if feature_val<=tree.threshold:
#             return self.make_prediction(x, tree.left)
#         else:
#             return self.make_prediction(x, tree.right)

# X = df2.iloc[:, :-1].values
# Y = df2.iloc[:, -1].values.reshape(-1,1)
# from sklearn.model_selection import train_test_split
# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2, random_state=41)

# classifier = DecisionTreeClassifier(min_samples_split=3, max_depth=3)
# start_train = time.perf_counter()
# classifier.fit(X_train,Y_train)
# end_train =time.perf_counter()
# classifier.print_tree()

# start_test = time.perf_counter()
# Y_pred = classifier.predict(X_test)
# end_test = time.perf_counter()
# from sklearn.metrics import accuracy_score
# accuracy_score(Y_test, Y_pred)

# print("Training Time:", end_train - start_train)
# print("Testing Time:", end_test - start_test)